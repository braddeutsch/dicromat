I have an app that generates a single colorblind test image. It has sliders that allow me to adjust the RGB values of the foreground and background, the offset and variance of grayscale noise, the size and variance of the circle sizes, and other parameters.

My son has Mild deuteranomaly or protanomaly (red–green spectrum weakness). I have normal color vision. I want to discover an image that is easy for him to distinguish and hard for me to distinguish. What parameter space should i explore?
ChatGPT said:
Thought for 10s

Great — you already have the right toybox (RGB sliders, noise, circle sizes). I’ll give you a practical, experiment-first plan plus a compact set of metrics and parameter ranges so you can search the right part of the space systematically and efficiently. The plan works both for human-in-the-loop (you + your son testing candidates) and for a quick automated search if you want to add that later.

Quick goal restatement (to keep us honest)

Find an image I made from your app parameters such that:

Your son with mild deuteran/protanomaly can easily see the embedded pattern, and

You (normal trichromat) cannot (or it’s very hard).

We’ll explore parameter settings that tend to produce that outcome and give you procedures to find winners.

Key principles to guide the search (conceptual)

Exploit differences in cone-channel mappings: dichromats collapse the L–M axis differently than normals. That means some pairs of RGB values that look similar to trichromats can map to different percepts for dichromats — those are the target pairs.

Mask luminance cues for normals: normal observers often rely on luminance/texture to detect shape. By matching luminance between foreground and background and adding grayscale noise, you make shape detection harder for normals — but a chromatic difference that dichromats perceive can still reveal the shape.

Work in device-corrected linear space where possible: match perceived luminance (not raw R/G/B) to avoid accidental luminance differences cluing normals. A simple luminance proxy is Y ≈ 0.2126 R + 0.7152 G + 0.0722 B (sRGB linear values).

Search along inverse R/G moves: change R and G in opposite directions while keeping Y constant — that creates chromatic shifts without changing luminance, which is a tight space to find metamers or near-metamers for trichromats.

Make the pattern subtle in chroma to normals but stronger in the dichromat channel: that’s the objective — small Δ for trichromats, larger Δ for dichromats.

Concrete parameter space to explore (use these ranges as your sliders’ test grid)

All color sliders in 0–255 (sRGB). When using the formulas below, convert 0–255 → 0–1 and linearize sRGB before using luminance formula (see step below).

Foreground RGB: start from mid-tones —R,G,B in [80, 200].

Background RGB: same coarse range [80, 200] but keep initial luminance matched to foreground.

Circle size (if pattern is circles): small→medium: 6–40 px. (Smaller sizes hide shape for normals more; larger make it easier for both.)

Circle-size variance (randomness): 0%–60% of mean size. Try 20%–40% for best camo.

Grayscale noise offset (mean luminance bias): -0.08 → +0.08 (in normalized 0–1 scale) — keep small so you don’t reveal luminance cues.

Grayscale noise variance (contrast): 0 → 0.25 (0 is no noise). Try 0.05–0.18 for masking without drowning image.

Pattern density (fraction of fg pixels): 10%–60% depending on task difficulty. Start 20%–35%.

Border smoothness / anti-aliasing: enable it, but keep edge contrast low to reduce luminance edges.

Practical measurement utilities (add these or compute as you test)

Perceived luminance (Y) — compute to match foreground/background:

Convert sRGB 0–255 → s (0–1). Linearize (if s ≤ 0.04045: s_lin = s/12.92; else s_lin = ((s+0.055)/1.055)^2.4).

Y ≈ 0.2126 R_lin + 0.7152 G_lin + 0.0722 B_lin.

Ensure |Y_fg - Y_bg| < 0.01 (tight) to avoid luminance cues for normals.

Optional: Dichromat simulation preview — if you add a quick Machado/Brettel transform you can preview how the image looks to a deutan/protan. (Later I can give code snippets.) For now you can eyeball with your son.

Score function (human-in-the-loop) — have a small form:

Son: Did you see the shape? (Y/N), time to find (s).

Parent: Did you see the shape? (Y/N), time to find (s).
Log responses.

Simple manual search protocol (fast, human-in-the-loop)

Pick a base background color: mid-grayish-neutral (e.g., RGB ≈ (150,150,150) linearized). Compute Y_base.

Set foreground luminance to match: choose candidate FG RGB with Y ≈ Y_base using the luminance formula. A quick way: fix B and solve for G when R is changed to keep Y constant:
G_lin = (Y_base - 0.2126*R_lin - 0.0722*B_lin) / 0.7152 (convert back to sRGB).
This gives a family of FG colors with identical luminance but different chromaticities.

Sweep R up & G down (and vice versa) while keeping Y constant. For each candidate FG color:

Render pattern (circles) with small-to-medium size, moderate density, grayscale noise variance ~0.08.

Show to your son first (ask him to find the pattern). If he can see it easily, then show it to you. Keep candidates where son sees it but you don’t.

Modify noise: if you (normal) see it too easily, increase grayscale noise variance or increase circle-size variance to mask luminance/edge cues. If your son fails, reduce noise or increase chromatic contrast (move further in R/G sweep).

Log winners & nearby parameter tweaks and refine.

This human-guided sweep usually yields a handful of promising pairs within dozens of trials.

If you want to automate search (short recipe)

Define a loss on a small representative sample of pixels (or on single pixel pairs if you start simple):

loss = α * (ΔE_trichromat(img_fg, img_bg))^2  - β * (ΔE_dichromat(img_fg, img_bg))^2


Minimize loss. α should be large (we strongly enforce trichromat similarity), β should be moderate (we want dichromat separability).

ΔE_trichromat: compute CIE Lab difference (ΔE2000) after converting to Lab (convert sRGB→XYZ→Lab).

ΔE_dichromat: compute ΔE between simulated dichromat versions of the two colors. Use Machado or Brettel transform first, then to Lab.

Start with random-restarts or CMA-ES to search the 3–6 dimensional color space, constrained by luminance matching.

This approach is compact and will find candidate color pairs quickly. If you want, I can give a short Python snippet for this later.

Heuristic color choices that often work (starting points)

(You still must adapt to your son’s specific profile and device.)

Background ≈ neutral mid-gray: RGB ≈ (140–160, 140–160, 140–160) (ensure linearized Y match).

Foreground candidates (try these after converting to linearized and checking Y):

Increase R while decreasing G (keeping Y): R up by 10–40, G down by 10–40.

Or the inverse: G up and R down.

Avoid large blue (B) shifts unless you want to target tritan-like differences — your son is deut/prot so L–M axis moves are better.

These produce subtle hue shifts that can be invisible or near-invisible to you but more salient to his altered L–M mapping.

Pattern & noise design tips

Use many small items (circles) with random positions and sizes rather than one big crisp shape — edges and large luminance differences make it easy for normals.

Use moderate grayscale noise (not color noise) to hide luminance edges while preserving chromatic contrasts.

Use no hard black/white borders — keep edges anti-aliased and low contrast.

Consider using a low-saturation foreground (low chroma) that clings near gray for normals but maps to more separation under dichromat transforms.

Logging and iteration

Each test should record: FG RGB, BG RGB, noise variance, circle mean/variance, density, and both testers’ yes/no + response times.

Try batches of ~25 candidates per session (son fatigue is real). You’ll likely find winners within a few sessions.

Next steps I can help with (pick any)

Provide the small sRGB → linear → Y code snippet (JS/Swift/Python).

Give an implementation of a Machado dichromat simulation (short JS or Python) so you can preview dichromat view in-app.

Give a compact automated search script (Python) that runs the loss above and returns candidate RGB pairs.

Walk through converting your app’s sliders to work in linear space and automatically enforce luminance matching.

Tell me which of those you want and what platform your app uses (web/React Native/iOS/Android), and I’ll give runnable code or a direct how-to.
